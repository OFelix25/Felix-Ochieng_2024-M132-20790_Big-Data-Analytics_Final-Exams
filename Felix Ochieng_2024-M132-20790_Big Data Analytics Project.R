# Load required libraries 
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)

# PRACTICAL BIG DATA ANALYTICS SOLUTION USING R, UMU Case Study 
# Step 1: Creating a Large Student Dataset (Volume). To simulate the universityâ€™s large 
#historical dataset to predict academic performance,a synthetic dataset of 20,000 
#student records was generated. This dataset represents data collected from LMS usage, 
#attendance tracking, and examination systems.

set.seed(2026)

student_data <- data.frame(
  Student_ID = 1:20000,
  Gender = sample(c("Male", "Female"), 20000, replace = TRUE),
  Attendance = sample(40:100, 20000, replace = TRUE),
  LMS_Engagement = sample(1:100, 20000, replace = TRUE),
  Assignment_Score = sample(10:40, 20000, replace = TRUE),
  Exam_Score = sample(20:60, 20000, replace = TRUE)
)

student_data$Final_Result <- ifelse(
  student_data$Attendance >= 70 &
    student_data$Exam_Score >= 40 &
    student_data$LMS_Engagement >= 50,
  "Pass",
  "Fail"
)

write.csv(student_data, "student_data_final_project.csv", row.names = FALSE)

# The above data shows realistic student academic indicators and clearly shows a 
#Pass/Fail outcome for predictive modeling 

#Step 2: Installing and Loading Required R Packages (Scalable analytics support)
install.packages(c("data.table", "dplyr", "ggplot2", "caret"))

# Loading the packages
library(data.table)
library(dplyr)
library(ggplot2)
library(caret)

# Step 3: Loading the Big Dataset (Batch Processing)
student_data <- fread("student_data_final_project.csv")

#Step 4: Understanding the Data (Data understanding & validation)
# Checking structure 
str(student_data)

#Checking the data size 
nrow(student_data)

#Checking the data size
ncol(student_data)

#Viewing first few rows
head(student_data)

# Viewing data summary
summary(student_data)

# Step 5: Data Cleaning (Veracity Management. This ensures data accuracy and 
#removes incomplete records
colSums(is.na(student_data))
student_data <- na.omit(student_data)

#Step 6: Efficient Data Processing (Scalable Analytics). This creates a total score 
#and enables scalability for large data sets
student_data[, Total_Score := Assignment_Score + Exam_Score]

#Step 7: Descriptive Analytics (What Happened?). This gives a clear insight based on 
#the overall pass vs fail rates, average performance levels and 
#Gender-based performance trends
table(student_data$Final_Result)
mean(student_data$Exam_Score)
student_data[, .(Avg_Exam = mean(Exam_Score)), by = Gender]

#Step 8: Diagnostic Analytics (Why Did It Happen?). This gives an insight on the 
#relationship between attendance and performance and the influence of
#LMS engagement on exam scores
cor(student_data$Attendance, student_data$Exam_Score)
cor(student_data$LMS_Engagement, student_data$Exam_Score)
# From the above, the result shows a negative relationship between attendance and the 
#performance of students. This implies that as the student attendance
# decreases, this negatively affects their performance in class.
# However there is a positive relationship between LMS engagement and exam implying 
#that use of LMS engagement for learning increases the students' chances
# of performing well in the exams

#Step 9: Big Data Visualization (Visual Analytics)
ggplot(student_data, aes(x = Exam_Score)) +
  geom_histogram(binwidth = 5) +
  labs(title = "Exam Score Distribution")

ggplot(student_data, aes(x = Attendance, y = Exam_Score)) +
  geom_point(alpha = 0.3) +
  labs(title = "Attendance vs Exam Performance")

ggplot(student_data, aes(x = Final_Result)) +
  geom_bar() +
  labs(title = "Pass vs Fail Distribution")

#Step 10: Preparing Data for Prediction. The pass/fail has been coded as 1=Pass 
#and 0=Fail
student_data$Result_Num <- ifelse(student_data$Final_Result == "Pass", 1, 0)

#Step 11: Splitting the Data (Predictive analytics preparation technique)
set.seed(2026)

train_index <- createDataPartition(
  student_data$Result_Num,
  p = 0.7,
  list = FALSE
)

train_data <- student_data[train_index]
test_data  <- student_data[-train_index]

#Step 12: Building the Predictive Model. This uses predictive analytics techniques and 
#logistic regression statistical method 
model <- glm(
  Result_Num ~ Attendance + LMS_Engagement + Assignment_Score + Exam_Score,
  data = train_data,
  family = binomial
)

#Step 13: Understanding the Mode. This enables the data analyst to gain insights into the
#Significant predictors, direction of influence and
#model coefficients
summary(model)

#Step 14: Making Predictions
prob_predictions <- predict(model, test_data, type = "response")
class_predictions <- ifelse(prob_predictions >= 0.5, 1, 0)

#Step 15: Model Evaluation (Accuracy). This helps in model validation

confusionMatrix(
  as.factor(class_predictions),
  as.factor(test_data$Result_Num)
)

#Step 16: Visualizing Prediction Results
prediction_results <- data.frame(
  Actual = test_data$Result_Num,
  Predicted = class_predictions
)

ggplot(prediction_results, aes(x = Actual, fill = as.factor(Predicted))) +
  geom_bar(position = "dodge") +
  labs(
    title = "Actual vs Predicted Academic Performance",
    fill = "Predicted"
  )
# Interpretation
#From the output, the predictive model achieved an accuracy of 93.07%, significantly 
#outperforming baseline classification. WWith a sensitivity of 96.9%, the system is 
#highly effective in identifying academically at-risk students, enabling timely 
#intervention strategies.This demonstrates the value of Big Data Analytics in transforming 
#historical educational data into actionable decision-support insights.

#Conclusion 
# This practical implementation demonstrates how Big Data Analytics techniques can be
#applied using R to analyze large-scale historical student at UMU. By combining descriptive,
#diagnostic and predictive analytics, the solution enables early identification of 
#academically at-risk students, student failure rates and supports data-driven academic 
#decision-making. The approach aligns with Big Data principles of volume, scalability, 
#prediction, validation and value extraction. 

